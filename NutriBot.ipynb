{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc64245",
   "metadata": {},
   "source": [
    "# NutriBot: Smart Nutrition Assistant for Personalized Dietary Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aCGwaV9jW0jI",
   "metadata": {
    "id": "aCGwaV9jW0jI"
   },
   "source": [
    "## 1. Install and Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8526d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "6fc8526d",
    "outputId": "249eba66-cee1-4937-e38d-b3dc34607060"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import fitz\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from deep_translator import GoogleTranslator\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display\n",
    "import gradio as gr\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8F3-Bm9XCWZ",
   "metadata": {
    "id": "q8F3-Bm9XCWZ"
   },
   "source": [
    "## 2. Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f157c",
   "metadata": {
    "id": "573f157c"
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "dotenv_path = find_dotenv()\n",
    "if dotenv_path:\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(f\"‚úÖ .env loaded from: {dotenv_path}\")\n",
    "else:\n",
    "    print(\"‚ùå .env file not found\")\n",
    "\n",
    "# Enable LangChain advanced tracing and evaluation (LangSmith)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"NutriBot Evaluation\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "# Set LangSmith API endpoint and API key\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(\"üîë LangSmith API Key:\", LANGCHAIN_API_KEY)\n",
    "\n",
    "# Load OpenAI API Key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"üîë OpenAI API Key:\", OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3441e",
   "metadata": {},
   "source": [
    "## 3. Load and Parse Datasets (PDF Books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faed489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìñ Extracting text from all 3 PDF books...\")\n",
    "\n",
    "# Dictionary of book sources and file paths\n",
    "book_files = {\n",
    "    \"Krause\": \"Janice L. Raymond, Kelly Morrow - Krause and Mahan‚Äôs Food and the Nutrition Care Process, 16E-Elsevier (2022).pdf\",\n",
    "    \"PocketGuide\": \"Essential_Pocket_Guide_Clinical_Nutrition.pdf\",\n",
    "    \"Pathophysiology\": \"Marcia Nelms, Kathryn P. Sucher - Nutrition Therapy and Pathophysiology-Cengage Learning (2019).pdf\"\n",
    "}\n",
    "\n",
    "# List to hold extracted paragraphs\n",
    "all_paragraphs = []\n",
    "\n",
    "# Loop through each book\n",
    "for source, file in book_files.items():\n",
    "    try:\n",
    "        print(f\"üîπ Reading: {file}\")\n",
    "        doc = fitz.open(file)  # Open PDF\n",
    "\n",
    "        book_text = \"\"\n",
    "        for page in doc:\n",
    "            book_text += page.get_text()  # Extract text page by page\n",
    "        doc.close()\n",
    "\n",
    "        # Split the raw text into meaningful paragraphs\n",
    "        paragraphs = [p.strip() for p in book_text.split(\"\\n\\n\") if len(p.strip()) > 50]\n",
    "\n",
    "        # Tag each paragraph with its source\n",
    "        for para in paragraphs:\n",
    "            all_paragraphs.append({\"text\": para, \"source\": source})\n",
    "\n",
    "        print(f\"‚úÖ Extracted {len(paragraphs)} paragraphs from {source}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "book_df = pd.DataFrame(all_paragraphs)\n",
    "\n",
    "# Print final stats and sample\n",
    "print(f\"‚úÖ Combined final dataset shape: {book_df.shape}\")\n",
    "\n",
    "if len(book_df) > 0:\n",
    "    print(book_df.sample(min(5, len(book_df))))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wUCDofwpXql3",
   "metadata": {
    "id": "wUCDofwpXql3"
   },
   "source": [
    "## 4. Build Vector Store (ChromaDB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rzc2XHZFXsWs",
   "metadata": {
    "id": "rzc2XHZFXsWs"
   },
   "outputs": [],
   "source": [
    "# Convert to LangChain Documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=str(row[\"text\"]),\n",
    "        metadata={\"source\": row[\"source\"]}\n",
    "    )\n",
    "    for _, row in book_df.iterrows()\n",
    "]\n",
    "\n",
    "# Chunk Text into Smaller Passages\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"‚úÖ Split into {len(texts)} text chunks.\")\n",
    "\n",
    "# Generate Embeddings\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Store Embeddings in Chroma VectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    texts,\n",
    "    embedding,\n",
    "    persist_directory=\"chromaadb/\",\n",
    "    collection_metadata={\"hnsw:space\": \"ip\"}\n",
    ")\n",
    "vectorstore.persist()\n",
    "print(\"‚úÖ Chroma vectorstore created and persisted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be6f7c",
   "metadata": {},
   "source": [
    "## 5. Prompt Template & Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e2703",
   "metadata": {},
   "source": [
    "### Create memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80507390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory object to retain conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd2c29",
   "metadata": {},
   "source": [
    "### Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured prompt template to prevent hallucination\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful, honest, and well-trained nutrition assistant.\n",
    "\n",
    "Your task is to answer questions strictly based on the given context. Follow these instructions carefully:\n",
    "\n",
    "1. **Use ONLY the information provided in the context.**\n",
    "   - Do not use any external knowledge.\n",
    "   - Do not assume or guess facts that are not explicitly stated in the context.\n",
    "\n",
    "2. **If the context does not contain the answer:**\n",
    "   - Say: \"I'm sorry, I don't have enough information to answer that.\"\n",
    "   - Do NOT attempt to generate or hallucinate an answer.\n",
    "\n",
    "3. **Answer clearly and concisely.**\n",
    "   - Focus on factual, evidence-based information.\n",
    "   - Avoid speculation or over-explaining.\n",
    "\n",
    "4. **Maintain a neutral and professional tone.**\n",
    "   - Avoid overly casual or emotional responses.\n",
    "\n",
    "---------------------\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "---------------------\n",
    "Your Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e323f",
   "metadata": {},
   "source": [
    "## 6. Load LLM and Create RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vectorstore for similarity-based context fetching\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Load OpenAI GPT-4o model for answering questions\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95537fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(user_question, context):\n",
    "    if not context or len(context.strip()) < 50:\n",
    "        return \"‚ùå I'm sorry, I don't have enough information to answer that.\"\n",
    "\n",
    "    # Load memory\n",
    "    memory_variables = memory.load_memory_variables({})\n",
    "    chat_history_list = memory_variables[\"chat_history\"]\n",
    "\n",
    "    # Convert chat_history to string\n",
    "    chat_history_str = \"\\n\".join([\n",
    "        f\"{msg.type.capitalize()}: {msg.content}\" for msg in chat_history_list\n",
    "    ])\n",
    "\n",
    "    # Format prompt\n",
    "    final_prompt = prompt_template.format(\n",
    "        chat_history=chat_history_str,\n",
    "        context=context,\n",
    "        question=user_question\n",
    "    )\n",
    "\n",
    "    # Invoke LLM manually\n",
    "    response = llm.invoke(final_prompt)\n",
    "\n",
    "    # Save this interaction to memory\n",
    "    memory.chat_memory.add_user_message(user_question)\n",
    "    memory.chat_memory.add_ai_message(response.content)\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f12b1b",
   "metadata": {},
   "source": [
    "## 7. QA Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_tool_wrapper(question_text):\n",
    "    try:\n",
    "        # 1. ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ≥ŸäÿßŸÇ ŸÖŸÜ ChromaDB\n",
    "        context_docs = retriever.get_relevant_documents(question_text)\n",
    "\n",
    "        # 2. ŸÅŸÑÿ™ÿ±ÿ© ÿ£ÿ∞ŸÉŸâ: ŸÜÿ≥ŸÖÿ≠ ÿ®ÿπÿ®ÿßÿ±ÿßÿ™ ÿπÿßŸÖÿ© ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÉŸÑŸÖÿßÿ™ ŸÖŸÅÿ™ÿßÿ≠Ÿäÿ© ŸÖŸáŸÖÿ©\n",
    "        important_keywords = [\"blood pressure\", \"hypertension\", \"sodium\", \"salt\", \"diet\", \"avoid\"]\n",
    "        filtered_docs = [\n",
    "            doc for doc in context_docs\n",
    "            if len(doc.page_content.strip()) > 100 or \n",
    "            any(kw in doc.page_content.lower() for kw in important_keywords)\n",
    "        ]\n",
    "\n",
    "        # 3. ÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ŸÖÿ≠ÿ™ŸàŸâ ÿ®ÿπÿØ ÿßŸÑŸÅŸÑÿ™ÿ±ÿ©\n",
    "        if not filtered_docs:\n",
    "            return \"‚ùå I'm sorry, I don't have enough information from trusted sources to answer that.\"\n",
    "\n",
    "        # 4. ÿØŸÖÿ¨ ÿßŸÑÿ≥ŸäÿßŸÇ\n",
    "        context = \"\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "\n",
    "        # 5. ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n",
    "        chat_history_list = memory.load_memory_variables({})[\"chat_history\"]\n",
    "        chat_history_str = \"\\n\".join([\n",
    "            f\"{msg.type.capitalize()}: {msg.content}\" for msg in chat_history_list\n",
    "        ])\n",
    "\n",
    "        # 6. ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ®ÿ±ŸàŸÖÿ®ÿ™\n",
    "        final_prompt = prompt_template.format(\n",
    "            chat_history=chat_history_str,\n",
    "            context=context,\n",
    "            question=question_text\n",
    "        )\n",
    "\n",
    "        # 7. ÿßÿ≥ÿ™ÿØÿπÿßÿ° ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
    "        response = llm.invoke(final_prompt)\n",
    "\n",
    "        # 8. ŸÅŸÑÿ™ÿ±ÿ© ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≠ÿ∏Ÿàÿ±ÿ© ÿ´ŸÇÿßŸÅŸäŸãÿß\n",
    "        banned_terms = [\n",
    "            r\"\\b(pork)\\b\",\n",
    "            r\"\\b(bacon)\\b\",\n",
    "            r\"\\b(ham)\\b\",\n",
    "            r\"\\b(prosciutto)\\b\",\n",
    "            r\"\\b(sausage made from pork)\\b\"\n",
    "        ]\n",
    "\n",
    "        # ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ ŸÉŸÑ ŸÉŸÑŸÖÿ© ÿ®ÿ¨ŸÖŸÑÿ© ŸÅÿßÿ±ÿ∫ÿ© ÿ£Ÿà ÿ™ÿ±ŸÉ ÿßŸÑŸÉŸÑŸÖÿ© ÿ™ŸÜÿ≠ÿ∞ŸÅ ÿ®ÿ≥ŸÑÿßÿ≥ÿ©\n",
    "        for term in banned_terms:\n",
    "            response.content = re.sub(term, \"\", response.content, flags=re.IGNORECASE)\n",
    "\n",
    "        # ÿ™ŸÜÿ∏ŸäŸÅ ÿ£Ÿä ŸÅÿ±ÿßÿ∫ ÿ≤ÿßÿ¶ÿØ ÿ£Ÿà ŸÅŸàÿßÿµŸÑ ÿ∫Ÿäÿ± ŸÖŸÜÿ∑ŸÇŸäÿ©\n",
    "        response.content = re.sub(r\"\\s{2,}\", \" \", response.content)\n",
    "        response.content = re.sub(r\"\\s+([.,])\", r\"\\1\", response.content)\n",
    "\n",
    "        # 9. ÿ≠ŸÅÿ∏ ŸÅŸä ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©\n",
    "        memory.chat_memory.add_user_message(question_text)\n",
    "        memory.chat_memory.add_ai_message(response.content)\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"System error: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac8d4",
   "metadata": {},
   "source": [
    "## 8. Image Captioning with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GoogleTranslator to translate text between English and Arabic\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def image_caption(image_path):\n",
    "    try:\n",
    "        image = Image.open(str(image_path)).convert('RGB')\n",
    "        inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "        out = blip_model.generate(**inputs)\n",
    "        return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Image processing error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cdee7",
   "metadata": {},
   "source": [
    "## 9. Translation Between Arabic and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GoogleTranslator to translate text between English and Arabic\n",
    "def translate(text, source_lang=\"en\", target_lang=\"ar\"):\n",
    "    try:\n",
    "        return GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6330f",
   "metadata": {},
   "source": [
    "## 10. Audio Transcription with Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349756bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper models for both English and Arabic transcription\n",
    "whisper_model_ar = WhisperModel(\"large\", compute_type=\"auto\", device=\"cpu\")\n",
    "whisper_model_en = WhisperModel(\"medium\", compute_type=\"auto\", device=\"cpu\")\n",
    "\n",
    "def transcribe_audio(audio_path, language):\n",
    "    try:\n",
    "        if language == \"ar\":\n",
    "            model = whisper_model_ar\n",
    "        elif language == \"en\":\n",
    "            model = whisper_model_en\n",
    "        else:\n",
    "            return \"‚ùå Unsupported language. Use 'ar' or 'en'.\"\n",
    "\n",
    "        segments, _ = model.transcribe(audio_path, language=language)\n",
    "        return \" \".join([seg.text for seg in segments])\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return None\n",
    "\n",
    "def handle_arabic_question(question_ar):\n",
    "    question_en = translate(question_ar, \"ar\", \"en\")\n",
    "    answer_en = qa_tool_wrapper(question_en)\n",
    "    return translate(answer_en, \"en\", \"ar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ZSJ090MXvda",
   "metadata": {
    "id": "6ZSJ090MXvda"
   },
   "source": [
    "## 11. LangChain Tools & Agent Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cfb36",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kiUHBAWIXzJb",
   "metadata": {
    "id": "kiUHBAWIXzJb"
   },
   "outputs": [],
   "source": [
    "# Define tools that the agent can call (e.g., QA, image, audio, translation)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Disease_Information\",\n",
    "        func=qa_tool_wrapper,\n",
    "        description=\"For questions about disease definitions, causes, and risk factors. Uses conversation history.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Symptom_Checker\",\n",
    "        func=qa_tool_wrapper,\n",
    "        description=\"For checking symptoms related to specific conditions. Considers previous discussion.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Treatment_Advice\",\n",
    "        func=qa_tool_wrapper,\n",
    "        description=\"Provides evidence-based treatment recommendations. Maintains context.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Nutrition_Recommendation\",\n",
    "        func=qa_tool_wrapper,\n",
    "        description=\"Suggests personalized nutrition plans based on health conditions and history.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"General_Nutrition_QA\",\n",
    "        func=qa_tool_wrapper,\n",
    "        description=\"Answers general nutrition questions while remembering conversation context.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Image_Analysis\",\n",
    "        func=image_caption,\n",
    "        description=\"Analyzes food images and generates descriptions for follow-up questions.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Audio_Transcription\",\n",
    "        func=transcribe_audio,\n",
    "        description=\"Transcribes audio in Arabic or English for use in conversation.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Translation_Service\",\n",
    "        func=lambda text: translate(text, source_lang=\"auto\", target_lang=\"en\"),\n",
    "        description=\"Translates between Arabic and English while preserving context.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ba672",
   "metadata": {},
   "source": [
    "### Initialize conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    early_stopping_method=\"force\"  # ‚úÖ ŸäŸÖŸÜÿπ GPT ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿπÿ±ŸÅ ÿ£Ÿä ÿ£ÿØÿßÿ©\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ea9eb",
   "metadata": {},
   "source": [
    "## 12. LangSmith Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate responses using LangSmith (latency, cost, hallucination detection)\n",
    "client = Client()\n",
    "\n",
    "def evaluate_with_langsmith(user_question_en):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response_dict = agent_executor.invoke({\"input\": user_question_en})\n",
    "            end_time = time.time()\n",
    "\n",
    "        response = response_dict[\"output\"]\n",
    "        latency = round(end_time - start_time, 2)\n",
    "        cost = round(cb.total_cost, 4)\n",
    "\n",
    "        # Simple hallucination/relevance check (customize as needed)\n",
    "        irrelevant_keywords = [\n",
    "            \"cloud\", \"server\", \"AI model\", \"database\", \"networking\", \"API\",\n",
    "            \"programming\", \"code\", \"machine learning\", \"software\"\n",
    "        ]\n",
    "        hallucinated = any(keyword in response.lower() for keyword in irrelevant_keywords)\n",
    "        relevance = not hallucinated\n",
    "\n",
    "        # Log evaluation summary\n",
    "        print(f\"üìä Relevance: {relevance}, üëª Hallucination: {hallucinated}\")\n",
    "        print(f\"‚è±Ô∏è Latency: {latency}s, üí∞ Cost: ${cost}\")\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")\n",
    "        return \"‚ùå Sorry, an error occurred during evaluation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a601c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests on QA pairs and display metrics in DataFrame\n",
    "qa_pairs = [\n",
    "    { \"question\": \"What is IBS?\" },\n",
    "    { \"question\": \"How many calories are in an avocado?\" },\n",
    "    { \"question\": \"Best healthy breakfast for hypertension?\" },\n",
    "    # Hallucination test questions\n",
    "    { \"question\": \"Is eating glass good for digestion?\" },\n",
    "    { \"question\": \"Does eating magnets align your blood flow?\" },\n",
    "    { \"question\": \"Can plugging your ears help reduce sugar levels?\" },\n",
    "]\n",
    "\n",
    "# Collect evaluation results\n",
    "evaluation_results = []\n",
    "\n",
    "# Evaluate each question\n",
    "for item in qa_pairs:\n",
    "    question_text = item[\"question\"]\n",
    "    print(f\"\\nüîç Testing: {question_text}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    with get_openai_callback() as cb:\n",
    "        try:\n",
    "            response_dict = agent_executor.invoke({\"input\": question_text})\n",
    "            end_time = time.time()\n",
    "\n",
    "            response = response_dict[\"output\"]\n",
    "            latency = round(end_time - start_time, 2)\n",
    "            cost = round(cb.total_cost, 4)\n",
    "\n",
    "            # Simple hallucination check\n",
    "            hallucinated = any(word in response.lower() for word in [\n",
    "                \"motor oil\", \"fishtea\", \"ai model\", \"cloud\", \"network\", \"code\"\n",
    "            ])\n",
    "            relevance = not hallucinated\n",
    "\n",
    "            # Store the result\n",
    "            evaluation_results.append({\n",
    "                \"Question\": question_text,\n",
    "                \"Answer\": response,\n",
    "                \"Relevance\": relevance,\n",
    "                \"Hallucination\": hallucinated,\n",
    "                \"Latency (s)\": latency,\n",
    "                \"Cost ($)\": cost\n",
    "            })\n",
    "\n",
    "            print(f\"üìä Relevance: {relevance}, üëª Hallucination: {hallucinated}\")\n",
    "            print(f\"‚è±Ô∏è Latency: {latency}s, üí∞ Cost: ${cost}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during evaluation: {e}\")\n",
    "            evaluation_results.append({\n",
    "                \"Question\": question_text,\n",
    "                \"Answer\": \"ERROR\",\n",
    "                \"Relevance\": False,\n",
    "                \"Hallucination\": True,\n",
    "                \"Latency (s)\": None,\n",
    "                \"Cost ($)\": None\n",
    "            })\n",
    "\n",
    "# Show the evaluation results in a DataFrame\n",
    "df_eval = pd.DataFrame(evaluation_results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce96944",
   "metadata": {},
   "source": [
    "## 13. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d144a7f",
   "metadata": {},
   "source": [
    "### Main Chatbot Logic for Handling Text, Voice, and Image Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_logic(message, input_type, uploaded_image, audio_path, health_condition, language_choice):\n",
    "    user_question = \"\"\n",
    "    transcription_display = None\n",
    "\n",
    "    if input_type == \"Text\":\n",
    "        user_question = message\n",
    "\n",
    "    elif input_type == \"Voice\":\n",
    "        if audio_path is None:\n",
    "            return \"‚ùå No voice provided.\", None\n",
    "        language_code = \"ar\" if language_choice == \"Arabic\" else \"en\"\n",
    "        try:\n",
    "            user_question = transcribe_audio(audio_path, language=language_code)\n",
    "            transcription_display = user_question\n",
    "            if not user_question or len(user_question.strip()) < 3:\n",
    "                return \"‚ùå The transcription was empty or unclear. Please try speaking again.\", None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in transcription: {e}\")\n",
    "            return \"‚ùå Failed to transcribe the audio. Please try again.\", None\n",
    "\n",
    "    elif input_type == \"Image\":\n",
    "        if not uploaded_image:\n",
    "            return \"‚ùå No image uploaded.\", None\n",
    "        food_description = image_caption(uploaded_image)\n",
    "        if not food_description or \"error\" in food_description.lower():\n",
    "            return \"‚ùå Could not analyze the image properly.\", None\n",
    "        user_question = f\"Is {food_description} good to eat?\"\n",
    "        transcription_display = food_description\n",
    "\n",
    "    if health_condition and health_condition.strip() != \"\":\n",
    "        keywords = [\n",
    "            # English variations\n",
    "            \"eat\", \"can i eat\", \"should i eat\", \"avoid\", \"safe to eat\",\n",
    "            \"food\", \"foods\", \"meal\", \"diet\", \"drink\", \"nutrition\",\n",
    "            \"what should i eat\", \"what not to eat\", \"good for\", \"bad for\",\n",
    "            \"breakfast\", \"lunch\", \"dinner\", \"snack\", \"healthy\", \"unhealthy\",\n",
    "\n",
    "            # Arabic variations\n",
    "            \"ÿ£ŸÉŸÑ\", \"ÿ™ŸÜÿßŸàŸÑ\", \"ŸáŸÑ ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ\", \"ŸáŸÑ ŸäŸÖŸÉŸÜŸÜŸä\", \"ŸáŸÑ Ÿäÿ¨Ÿàÿ≤\", \"ŸäŸÜŸÅÿπ ÿ£ŸÉŸÑ\",\n",
    "            \"ŸÖÿß ÿßŸÑÿ£ÿ∑ÿπŸÖÿ©\", \"ŸÖÿß ÿßŸÑÿ£ŸÉŸÑ\", \"ŸÖÿß ÿßŸÑÿ∑ÿπÿßŸÖ\", \"ÿ™ÿ¨ŸÜÿ®\", \"ÿ∂ÿßÿ±\", \"ŸÖŸÅŸäÿØ\",\n",
    "            \"ÿ∫ÿØÿßÿ°\", \"ŸÅÿ∑Ÿàÿ±\", \"ÿπÿ¥ÿßÿ°\", \"Ÿàÿ¨ÿ®ÿ©\", \"ÿ¥ÿ±ÿ®\", \"ŸÖÿ¥ÿ±Ÿàÿ®\"\n",
    "        ]\n",
    "    \n",
    "    question_lower = user_question.lower()\n",
    "    if any(kw in question_lower for kw in keywords):\n",
    "        user_question += f\" I have {health_condition}.\"\n",
    "\n",
    "\n",
    "    user_question_en = user_question\n",
    "    if language_choice == \"Arabic\":\n",
    "        user_question_en = translate(user_question, source_lang=\"ar\", target_lang=\"en\")\n",
    "\n",
    "    try:\n",
    "        current_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to load chat history: {e}\")\n",
    "        current_history = []\n",
    "\n",
    "    try:\n",
    "        response_en = agent_executor.invoke({\"input\": user_question_en})['output']\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Agent error: {e}\")\n",
    "        return \"‚ùå Sorry, something went wrong while processing your question.\", transcription_display\n",
    "\n",
    "    irrelevant_keywords = [\n",
    "        \"cloud\", \"server\", \"AI model\", \"database\", \"networking\", \"API\",\n",
    "        \"programming\", \"code\", \"machine learning\", \"software\"\n",
    "    ]\n",
    "    if any(keyword in response_en.lower() for keyword in irrelevant_keywords):\n",
    "        response_en = \"‚ùå I'm sorry, that doesn't seem related to food or health. Please ask a nutrition-related question.\"\n",
    "\n",
    "    response = response_en\n",
    "    if language_choice == \"Arabic\" and user_question != user_question_en:\n",
    "        response = translate(response_en, source_lang=\"en\", target_lang=\"ar\")\n",
    "\n",
    "    return response, transcription_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a120c",
   "metadata": {},
   "source": [
    "###  Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc92acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_theme_css = \"\"\"\n",
    "<style>\n",
    "@font-face {\n",
    "    font-family: 'Dubai';\n",
    "    src: url('Dubai-Medium.ttf') format('truetype');\n",
    "    font-weight: normal;\n",
    "    font-style: normal;\n",
    "}\n",
    "\n",
    "body, .gradio-container {\n",
    "    background-color: #ffffff !important;\n",
    "    font-family: 'Dubai', -apple-system, BlinkMacSystemFont, sans-serif !important;\n",
    "}\n",
    "\n",
    ".gradio-container {\n",
    "    width: 100% !important;\n",
    "    max-width: 1200px !important;\n",
    "    margin: 0 auto !important;\n",
    "    padding: 20px !important;\n",
    "}\n",
    "\n",
    ".chat-container {\n",
    "    background-color: white;\n",
    "    border-radius: 8px;\n",
    "    box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "    height: calc(100vh - 200px) !important;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    overflow: hidden;\n",
    "}\n",
    "\n",
    "#chatbot {\n",
    "    flex-grow: 1;\n",
    "    overflow-y: auto;\n",
    "    padding: 20px;\n",
    "    background-color: #ffffff;\n",
    "    border-radius: 8px 8px 0 0;\n",
    "}\n",
    "\n",
    "/* User messages: background now light green, border green */\n",
    ".user-message {\n",
    "    background-color: #e0f7e9 !important;\n",
    "    border: 2px solid #7dd2a9 !important;\n",
    "    margin-left: auto;\n",
    "    margin-right: 10px;\n",
    "    border-bottom-right-radius: 0;\n",
    "    color: #333;\n",
    "    padding: 12px 16px;\n",
    "    margin-bottom: 16px;\n",
    "    border-radius: 8px;\n",
    "    line-height: 1.3;\n",
    "    max-width: 80%;\n",
    "    width: fit-content;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    ".bot-message {\n",
    "    background-color: #d5e7dd !important;\n",
    "    border: 2px solid #2e8b57 !important;\n",
    "    margin-right: auto;\n",
    "    margin-left: 10px;\n",
    "    border-bottom-left-radius: 0;\n",
    "    box-shadow: 0 1px 2px rgba(0,0,0,0.1);\n",
    "    color: #333;\n",
    "    padding: 12px 16px;\n",
    "    margin-bottom: 16px;\n",
    "    border-radius: 8px;\n",
    "    line-height: 1.3;\n",
    "    max-width: 80%;\n",
    "    width: fit-content;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    ".input-area {\n",
    "    padding: 16px;\n",
    "    background-color: white;\n",
    "    border-top: 1px solid #e5e5e5;\n",
    "    border-radius: 0 0 8px 8px;\n",
    "    display: flex;\n",
    "    gap: 10px;\n",
    "}\n",
    "\n",
    "#text-input {\n",
    "    flex-grow: 1;\n",
    "    border: 1px solid #e5e5e5;\n",
    "    border-radius: 20px;\n",
    "    padding: 12px 16px;\n",
    "    font-size: 16px;\n",
    "    outline: none;\n",
    "}\n",
    "\n",
    "#text-input:focus {\n",
    "    border-color: #7dd2a9;\n",
    "    box-shadow: 0 0 0 1px #7dd2a9;\n",
    "}\n",
    "\n",
    "button {\n",
    "    background-color: #7dd2a9 !important;\n",
    "    color: white !important;\n",
    "    border: none !important;\n",
    "    border-radius: 20px !important;\n",
    "    padding: 12px 20px !important;\n",
    "    font-size: 16px !important;\n",
    "    font-weight: 500 !important;\n",
    "    cursor: pointer !important;\n",
    "    transition: all 0.2s !important;\n",
    "}\n",
    "\n",
    "button:hover {\n",
    "    background-color: #6bc198 !important;\n",
    "}\n",
    "\n",
    "button:disabled {\n",
    "    background-color: #e5e5e5 !important;\n",
    "    color: #999 !important;\n",
    "}\n",
    "\n",
    ".options-area {\n",
    "    padding: 12px 16px;\n",
    "    background-color: white;\n",
    "    border-bottom: 1px solid #e5e5e5;\n",
    "    display: flex;\n",
    "    gap: 12px;\n",
    "    flex-wrap: wrap;\n",
    "}\n",
    "\n",
    ".dropdown, .textbox, .audio, .image {\n",
    "    margin-bottom: 0 !important;\n",
    "}\n",
    "\n",
    "@media (max-width: 768px) {\n",
    "    .gradio-container {\n",
    "        padding: 10px !important;\n",
    "    }\n",
    "    \n",
    "    .user-message, .bot-message {\n",
    "        max-width: 90%;\n",
    "        font-size: 18px;\n",
    "        padding: 10px 14px;\n",
    "    }\n",
    "    \n",
    "    .chat-container {\n",
    "        height: 65vh !important;\n",
    "    }\n",
    "    \n",
    "    #text-input, button {\n",
    "        font-size: 16px !important;\n",
    "        padding: 10px 14px !important;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce44527",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"NutriBot: Smart Nutrition Assistant\", \n",
    "               css=gr_theme_css) as demo:\n",
    "\n",
    "    with open(\"nutribotLogo.png\", \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode()\n",
    "    logo_data_url = f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "    gr.Markdown(f\"\"\"\n",
    "    <div style=\"\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        margin-bottom: 12px;\n",
    "    \">\n",
    "        <img \n",
    "            src=\"{logo_data_url}\" \n",
    "            alt=\"NutriBot Logo\" \n",
    "            style=\"max-width:200px; display:block;\"\n",
    "        />\n",
    "        <p style=\"margin: 8px 0 0; text-align: center; color: #666; font-size: 20px;\">\n",
    "            Type your question, speak, or upload food image to get healthy dietary suggestions\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"options-area\"]):\n",
    "        language_dropdown = gr.Dropdown(\n",
    "            choices=[\"English\", \"Arabic\"], \n",
    "            value=\"English\", \n",
    "            label=\"Language\",\n",
    "            interactive=True\n",
    "        )\n",
    "        health_input = gr.Textbox(\n",
    "            label=\"Health condition (optional)\", \n",
    "            placeholder=\"e.g., diabetes, high blood pressure...\",\n",
    "            interactive=True,\n",
    "            elem_id=\"health-input\"  \n",
    "        )\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        height=500,\n",
    "        show_label=False\n",
    "    )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"input-area\"]):\n",
    "        msg = gr.Textbox(\n",
    "            show_label=False,\n",
    "            placeholder=\"Type your nutrition question here...\",\n",
    "            container=False,\n",
    "            scale=7,\n",
    "            elem_id=\"text-input\"\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "    with gr.Accordion(\"Attach file (image or voice)\", open=False):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"<div style='text-align:center;'>Upload Image</div>\")\n",
    "                image_input = gr.Image(\n",
    "                    type=\"filepath\", \n",
    "                    interactive=True,\n",
    "                    show_label=False\n",
    "                )\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"<div style='text-align:center;'>Record or Upload Voice</div>\")\n",
    "                audio_input = gr.Audio(\n",
    "                    type=\"filepath\", \n",
    "                    interactive=True,\n",
    "                    show_label=False\n",
    "                )\n",
    "\n",
    "    status_message = gr.State(\"\")\n",
    "\n",
    "    def show_processing(chat_history, audio, lang_choice):\n",
    "        if audio is not None and lang_choice == \"Arabic\":\n",
    "            processing_msg = \"<div class='arabic-text'>ÿ¨ÿßÿ±Ÿä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿµŸàÿ™ ÿßŸÑÿπÿ±ÿ®Ÿä... ŸÇÿØ Ÿäÿ≥ÿ™ÿ∫ÿ±ŸÇ ÿ∞ŸÑŸÉ ŸàŸÇÿ™ ÿ£ÿ∑ŸàŸÑ ŸÖŸÜ ÿßŸÑŸÖÿπÿ™ÿßÿØ.</div>\"\n",
    "        elif audio is not None:\n",
    "            processing_msg = \"Processing audio... Please wait.\"\n",
    "        else:\n",
    "            return chat_history, \"\"\n",
    "            \n",
    "        chat_history.append((None, processing_msg))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def respond(message, chat_history, image, audio, health, lang_choice):\n",
    "        try:\n",
    "            if image is not None:\n",
    "                input_type = \"Image\"\n",
    "            elif audio is not None:\n",
    "                input_type = \"Voice\"\n",
    "            else:\n",
    "                input_type = \"Text\"\n",
    "\n",
    "            response, transcription_display = chatbot_logic(message, input_type, image, audio, health, lang_choice)\n",
    "\n",
    "            user_message = message if input_type == \"Text\" else None\n",
    "            if transcription_display:\n",
    "                if input_type == \"Image\":\n",
    "                    user_message = f\"üì∑ Image uploaded\"  # ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÜÿµ ŸÑŸäÿ¥ÿ®Ÿá ŸÉŸàÿØ 1\n",
    "                elif input_type == \"Voice\":\n",
    "                    user_message = f\"üé§ {transcription_display}\"\n",
    "\n",
    "            if lang_choice == \"Arabic\":\n",
    "                if user_message:\n",
    "                    user_message = f\"<div class='arabic-text'>{user_message}</div>\"\n",
    "                response = f\"<div class='arabic-text'>{response}</div>\"\n",
    "\n",
    "            chat_history.append((user_message, response))\n",
    "            return \"\", chat_history, None, None, \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error in respond(): {e}\")\n",
    "            error_message = \"‚ùå An error occurred while processing your question. Please try again.\"\n",
    "            if lang_choice == \"Arabic\":\n",
    "                error_message = \"<div class='arabic-text'>‚ùå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÖÿπÿßŸÑÿ¨ÿ© ÿ≥ÿ§ÿßŸÑŸÉ. ÿ≠ÿßŸàŸÑ ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ.</div>\"\n",
    "            chat_history.append((None, error_message))\n",
    "            return \"\", chat_history, None, None, \"\"\n",
    "\n",
    "    msg.submit(\n",
    "        fn=show_processing,\n",
    "        inputs=[chatbot, audio_input, language_dropdown],\n",
    "        outputs=[chatbot, status_message]\n",
    "    ).then(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, image_input, audio_input, health_input, language_dropdown],\n",
    "        outputs=[msg, chatbot, image_input, audio_input, status_message]\n",
    "    )\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=show_processing,\n",
    "        inputs=[chatbot, audio_input, language_dropdown],\n",
    "        outputs=[chatbot, status_message]\n",
    "    ).then(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, image_input, audio_input, health_input, language_dropdown],\n",
    "        outputs=[msg, chatbot, image_input, audio_input, status_message]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
